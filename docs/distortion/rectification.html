<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pano.distortion.rectification API documentation</title>
<meta name="description" content="Automated Rectification of Image …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pano.distortion.rectification</code></h1>
</header>
<section id="section-intro">
<p>Automated Rectification of Image.</p>
<p><a href="https://github.com/chsasank/Image-Rectification">https://github.com/chsasank/Image-Rectification</a></p>
<h2 id="references">References</h2>
<ol>
<li>Chaudhury, Krishnendu, Stephen DiVerdi, and Sergey Ioffe.
"Auto-rectification of user photos." 2014 IEEE International Conference on
Image Processing (ICIP). IEEE, 2014.</li>
<li>Bazin, Jean-Charles, and Marc Pollefeys. "3-line RANSAC for orthogonal
vanishing point detection." 2012 IEEE/RSJ International Conference on
Intelligent Robots and Systems. IEEE, 2012.</li>
</ol>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Automated Rectification of Image.

https://github.com/chsasank/Image-Rectification

References
----------
1.  Chaudhury, Krishnendu, Stephen DiVerdi, and Sergey Ioffe.
    &#34;Auto-rectification of user photos.&#34; 2014 IEEE International Conference on
    Image Processing (ICIP). IEEE, 2014.
2.  Bazin, Jean-Charles, and Marc Pollefeys. &#34;3-line RANSAC for orthogonal
    vanishing point detection.&#34; 2012 IEEE/RSJ International Conference on
    Intelligent Robots and Systems. IEEE, 2012.
&#34;&#34;&#34;

from typing import Optional, Tuple

from loguru import logger
import numpy as np
from skimage.feature import canny
from skimage.transform import probabilistic_hough_line
from skimage.transform import warp


def compute_edgelets(image, sigma=3):
  &#34;&#34;&#34;Create edgelets as in the paper.

  Uses canny edge detection and then finds (small) lines using probabilistic
  hough transform as edgelets.

  Parameters
  ----------
  image: ndarray
      Image for which edgelets are to be computed.
  sigma: float
      Smoothing to be used for canny edge detection.

  Returns
  -------
  locations: ndarray of shape (n_edgelets, 2)
      Locations of each of the edgelets.
  directions: ndarray of shape (n_edgelets, 2)
      Direction of the edge (tangent) at each of the edgelet.
  strengths: ndarray of shape (n_edgelets,)
      Length of the line segments detected for the edgelet.
  &#34;&#34;&#34;
  edges = canny(image, sigma)
  lines = probabilistic_hough_line(edges, line_length=3, line_gap=2)
  lines = np.array(lines)

  locations = np.average(lines, axis=1)
  directions = lines[:, 1, :] - lines[:, 0, :]
  strengths = np.linalg.norm(directions, ord=2, axis=1)
  directions = np.divide(directions, strengths.reshape([-1, 1]))

  return (locations, directions, strengths)


def edgelet_lines(edgelets):
  &#34;&#34;&#34;Compute lines in homogenous system for edgelets.

  Parameters
  ----------
  edgelets: tuple of ndarrays
      (locations, directions, strengths) as computed by `compute_edgelets`.

  Returns
  -------
  lines: ndarray of shape (n_edgelets, 3)
      Lines at each of edgelet locations in homogenous system.
  &#34;&#34;&#34;
  locations, directions, _ = edgelets
  normals = np.zeros_like(directions)
  normals[:, 0] = directions[:, 1]
  normals[:, 1] = -directions[:, 0]
  p = -np.sum(locations * normals, axis=1)
  lines = np.concatenate((normals, p[:, np.newaxis]), axis=1)

  return lines


def compute_votes(edgelets, model, threshold_inlier=5):
  &#34;&#34;&#34;Compute votes for each of the edgelet against a given vanishing point.

  Votes for edgelets which lie inside threshold are same as their strengths,
  otherwise zero.

  Parameters
  ----------
  edgelets: tuple of ndarrays
      (locations, directions, strengths) as computed by `compute_edgelets`.
  model: ndarray of shape (3,)
      Vanishing point model in homogenous cordinate system.
  threshold_inlier: float
      Threshold to be used for computing inliers in degrees. Angle between
      edgelet direction and line connecting the  Vanishing point model and
      edgelet location is used to threshold.

  Returns
  -------
  votes: ndarray of shape (n_edgelets,)
      Votes towards vanishing point model for each of the edgelet.

  &#34;&#34;&#34;
  vp = model[:2] / model[2]

  locations, directions, strengths = edgelets

  est_directions = locations - vp
  dot_prod = np.sum(est_directions * directions, axis=1)
  abs_prod = (np.linalg.norm(directions, axis=1) *
              np.linalg.norm(est_directions, axis=1))
  abs_prod[abs_prod == 0] = 1e-5

  cosine_theta = np.clip(dot_prod / abs_prod, a_min=-1.0, a_max=1.0)
  theta = np.arccos(np.abs(cosine_theta))

  theta_thresh = threshold_inlier * np.pi / 180

  return (theta &lt; theta_thresh) * strengths


def ransac_vanishing_point(edgelets, num_ransac_iter=2000, threshold_inlier=5):
  &#34;&#34;&#34;Estimate vanishing point using Ransac.

  Parameters
  ----------
  edgelets: tuple of ndarrays
      (locations, directions, strengths) as computed by `compute_edgelets`.
  num_ransac_iter: int
      Number of iterations to run ransac.
  threshold_inlier: float
      threshold to be used for computing inliers in degrees.

  Returns
  -------
  best_model: ndarray of shape (3,)
      Best model for vanishing point estimated.

  Reference
  ---------
  Chaudhury, Krishnendu, Stephen DiVerdi, and Sergey Ioffe.
  &#34;Auto-rectification of user photos.&#34; 2014 IEEE International Conference on
  Image Processing (ICIP). IEEE, 2014.
  &#34;&#34;&#34;
  locations, directions, strengths = edgelets
  lines = edgelet_lines(edgelets)

  num_pts = strengths.size

  arg_sort = np.argsort(-strengths)
  first_index_space = arg_sort[:num_pts // 5]
  second_index_space = arg_sort[:num_pts // 2]

  best_model = None
  best_votes = np.zeros(num_pts)

  for ransac_iter in range(num_ransac_iter):
    ind1 = np.random.choice(first_index_space)
    ind2 = np.random.choice(second_index_space)

    l1 = lines[ind1]
    l2 = lines[ind2]

    current_model = np.cross(l1, l2)

    if np.sum(current_model**2) &lt; 1 or current_model[2] == 0:
      # reject degenerate candidates
      continue

    current_votes = compute_votes(edgelets, current_model, threshold_inlier)

    if current_votes.sum() &gt; best_votes.sum():
      best_model = current_model
      best_votes = current_votes
      logger.trace(&#34;Current best model has {:.2f} votes at iteration {}&#34;,
                   current_votes.sum(), ransac_iter)

  logger.debug(&#39;Best model has {:.2f} votes&#39;, best_votes.sum())

  return best_model


def ransac_3_line(
    edgelets,
    focal_length,
    num_ransac_iter=2000,
    threshold_inlier=5) -&gt; Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
  &#34;&#34;&#34;Estimate orthogonal vanishing points using 3 line Ransac algorithm.

  Assumes camera has been calibrated and its focal length is known.

  Parameters
  ----------
  edgelets: tuple of ndarrays
      (locations, directions, strengths) as computed by `compute_edgelets`.
  focal_length: float
      Focal length of the camera used.
  num_ransac_iter: int
      Number of iterations to run ransac.
  threshold_inlier: float
      threshold to be used for computing inliers in degrees.

  Returns
  -------
  vp1: ndarray of shape (3,)
      Estimated model for first vanishing point.
  vp2: ndarray of shape (3,)
      Estimated model for second vanishing point, which is orthogonal to
      first vanishing point.

  Reference
  ---------
  Bazin, Jean-Charles, and Marc Pollefeys. &#34;3-line RANSAC for orthogonal
  vanishing point detection.&#34; 2012 IEEE/RSJ International Conference on
  Intelligent Robots and Systems. IEEE, 2012.
  &#34;&#34;&#34;
  locations, directions, strengths = edgelets
  lines = edgelet_lines(edgelets)

  num_pts = strengths.size

  arg_sort = np.argsort(-strengths)
  first_index_space = arg_sort[:num_pts // 5]
  second_index_space = arg_sort[:num_pts // 5]
  third_index_space = arg_sort[:num_pts // 2]

  invfsq = 1 / (focal_length**2)
  farr = np.array([invfsq, invfsq, 1.0])

  best_model = (None, None)
  best_votes = 0

  for ransac_iter in range(num_ransac_iter):
    ind1 = np.random.choice(first_index_space)
    ind2 = np.random.choice(second_index_space)
    ind3 = np.random.choice(third_index_space)

    l1 = lines[ind1]
    l2 = lines[ind2]
    l3 = lines[ind3]

    vp1 = np.cross(l1, l2)

    # The vanishing line polar to v1
    # h = np.dot(vp1, [1 / focal_length**2, 1 / focal_length**2, 1])
    # h = [vp1[0] * invfsq, vp1[1] * invfsq, vp1[2]]
    h = vp1 * farr
    vp2 = np.cross(h, l3)

    if np.sum(vp1**2) &lt; 1 or vp1[2] == 0:
      # reject degenerate candidates
      continue

    if np.sum(vp2**2) &lt; 1 or vp2[2] == 0:
      # reject degenerate candidates
      continue

    vp1_votes = compute_votes(edgelets, vp1, threshold_inlier)
    vp2_votes = compute_votes(edgelets, vp2, threshold_inlier)
    current_votes = (vp1_votes &gt; 0).sum() + (vp2_votes &gt; 0).sum()

    if current_votes &gt; best_votes:
      best_model = (vp1, vp2)
      best_votes = current_votes
      logger.debug(&#34;Current best model has {:.2f} votes at iteration {}&#34;,
                   current_votes, ransac_iter)

  return best_model


def reestimate_model(model, edgelets, threshold_reestimate=5):
  &#34;&#34;&#34;Reestimate vanishing point using inliers and least squares.

  All the edgelets which are within a threshold are used to reestimate model

  Parameters
  ----------
  model: ndarray of shape (3,)
      Vanishing point model in homogenous coordinates which is to be
      reestimated.
  edgelets: tuple of ndarrays
      (locations, directions, strengths) as computed by `compute_edgelets`.
      All edgelets from which inliers will be computed.
  threshold_inlier: float
      threshold to be used for finding inlier edgelets.

  Returns
  -------
  reestimated_model: ndarray of shape (3,)
      Reestimated model for vanishing point in homogenous coordinates.
  &#34;&#34;&#34;
  locations, directions, strengths = edgelets

  inliers = compute_votes(edgelets, model, threshold_reestimate) &gt; 0
  locations = locations[inliers]
  directions = directions[inliers]
  strengths = strengths[inliers]

  lines = edgelet_lines((locations, directions, strengths))

  a = lines[:, :2]
  b = -lines[:, 2]
  est_model = np.linalg.lstsq(a, b)[0]

  return np.concatenate((est_model, [1.]))


def remove_inliers(model, edgelets, threshold_inlier=10):
  &#34;&#34;&#34;Remove all inlier edgelets of a given model.

  Parameters
  ----------
  model: ndarray of shape (3,)
      Vanishing point model in homogenous coordinates which is to be
      reestimated.
  edgelets: tuple of ndarrays
      (locations, directions, strengths) as computed by `compute_edgelets`.
  threshold_inlier: float
      threshold to be used for finding inlier edgelets.

  Returns
  -------
  edgelets_new: tuple of ndarrays
      All Edgelets except those which are inliers to model.
  &#34;&#34;&#34;
  inliers = compute_votes(edgelets, model, threshold_inlier) &gt; 0
  locations, directions, strengths = edgelets
  locations = locations[~inliers]
  directions = directions[~inliers]
  strengths = strengths[~inliers]
  edgelets = (locations, directions, strengths)

  return edgelets


def compute_homography_and_warp(image, vp1, vp2, clip=True, clip_factor=3):
  &#34;&#34;&#34;Compute homography from vanishing points and warp the image.

  It is assumed that vp1 and vp2 correspond to horizontal and vertical
  directions, although the order is not assumed.
  Firstly, projective transform is computed to make the vanishing points go
  to infinity so that we have a fronto parallel view. Then, Computes affine
  transform  to make axes corresponding to vanishing points orthogonal.
  Finally, Image is translated so that the image is not missed. Note that
  this image can be very large. `clip` is provided to deal with this.

  Parameters
  ----------
  image: ndarray
      Image which has to be wrapped.
  vp1: ndarray of shape (3, )
      First vanishing point in homogenous coordinate system.
  vp2: ndarray of shape (3, )
      Second vanishing point in homogenous coordinate system.
  clip: bool, optional
      If True, image is clipped to clip_factor.
  clip_factor: float, optional
      Proportion of image in multiples of image size to be retained if gone
      out of bounds after homography.

  Returns
  -------
  warped_img: ndarray
      Image warped using homography as described above.
  &#34;&#34;&#34;
  # Find Projective Transform
  vanishing_line = np.cross(vp1, vp2)
  H = np.eye(3)
  H[2] = vanishing_line / vanishing_line[2]
  H = H / H[2, 2]

  # Find directions corresponding to vanishing points
  v_post1 = np.dot(H, vp1)
  v_post2 = np.dot(H, vp2)
  v_post1 = v_post1 / np.sqrt(v_post1[0]**2 + v_post1[1]**2)
  v_post2 = v_post2 / np.sqrt(v_post2[0]**2 + v_post2[1]**2)

  directions = np.array([[v_post1[0], -v_post1[0], v_post2[0], -v_post2[0]],
                         [v_post1[1], -v_post1[1], v_post2[1], -v_post2[1]]])

  thetas = np.arctan2(directions[0], directions[1])

  # Find direction closest to horizontal axis
  h_ind = np.argmin(np.abs(thetas))

  # Find positive angle among the rest for the vertical axis
  if h_ind // 2 == 0:
    v_ind = 2 + np.argmax([thetas[2], thetas[3]])
  else:
    v_ind = np.argmax([thetas[2], thetas[3]])

  A1 = np.array([
      [directions[0, v_ind], directions[0, h_ind], 0],
      [directions[1, v_ind], directions[1, h_ind], 0],
      [0, 0, 1],
  ])
  # Might be a reflection. If so, remove reflection.
  if np.linalg.det(A1) &lt; 0:
    A1[:, 0] = -A1[:, 0]

  A = np.linalg.inv(A1)

  # Translate so that whole of the image is covered
  inter_matrix = np.dot(A, H)

  cords = np.dot(inter_matrix,
                 [[0, 0, image.shape[1], image.shape[1]],
                  [0, image.shape[0], 0, image.shape[0]], [1, 1, 1, 1]])
  cords = cords[:2] / cords[2]

  tx = min(0, cords[0].min())
  ty = min(0, cords[1].min())

  max_x = cords[0].max() - tx
  max_y = cords[1].max() - ty

  if clip:
    # These might be too large. Clip them.
    max_offset = max(image.shape) * clip_factor / 2
    tx = max(tx, -max_offset)
    ty = max(ty, -max_offset)

    max_x = min(max_x, -tx + max_offset)
    max_y = min(max_y, -ty + max_offset)

  max_x = int(max_x)
  max_y = int(max_y)

  T = np.array([[1, 0, -tx], [0, 1, -ty], [0, 0, 1]])

  final_homography = np.dot(T, inter_matrix)

  warped_img = warp(image,
                    np.linalg.inv(final_homography),
                    output_shape=(max_y, max_x))

  return warped_img


def vis_edgelets(image, edgelets, show=True):
  &#34;&#34;&#34;Helper function to visualize edgelets.&#34;&#34;&#34;
  import matplotlib.pyplot as plt

  plt.figure(figsize=(10, 10))
  plt.imshow(image)
  locations, directions, strengths = edgelets

  for i in range(locations.shape[0]):
    xax = [
        locations[i, 0] - directions[i, 0] * strengths[i] / 2,
        locations[i, 0] + directions[i, 0] * strengths[i] / 2
    ]
    yax = [
        locations[i, 1] - directions[i, 1] * strengths[i] / 2,
        locations[i, 1] + directions[i, 1] * strengths[i] / 2
    ]

    plt.plot(xax, yax, &#39;r-&#39;)

  if show:
    plt.show()


def vis_model(image, model, show=True):
  &#34;&#34;&#34;Helper function to visualize computed model.&#34;&#34;&#34;
  import matplotlib.pyplot as plt

  edgelets = compute_edgelets(image)
  locations, directions, strengths = edgelets
  inliers = compute_votes(edgelets, model, 10) &gt; 0

  edgelets = (locations[inliers], directions[inliers], strengths[inliers])
  locations, directions, strengths = edgelets
  vis_edgelets(image, edgelets, False)
  vp = model / model[2]
  plt.plot(vp[0], vp[1], &#39;bo&#39;)

  for i in range(locations.shape[0]):
    xax = [locations[i, 0], vp[0]]
    yax = [locations[i, 1], vp[1]]
    plt.plot(xax, yax, &#39;b-.&#39;)

  if show:
    plt.show()


def rectify_image(image: np.ndarray,
                  clip_factor=6,
                  algorithm=&#39;independent&#39;,
                  reestimate=False):
  &#34;&#34;&#34;Rectified image with vanishing point computed using ransac.

  Parameters
  ----------
  image: ndarray
      Image which has to be rectified. Must be gray.
  clip_factor: float, optional
      Proportion of image in multiples of image size to be retained if gone
      out of bounds after homography.
  algorithm: one of {&#39;3-line&#39;, &#39;independent&#39;}
      independent ransac algorithm finds the orthogonal vanishing points by
      applying ransac twice.
      3-line algorithm finds the orthogonal vanishing points together, but
      assumes knowledge of focal length.
  reestimate: bool
      If ransac results are to be reestimated using least squares with
      inliers. Turn this off if getting bad results.

  Returns
  -------
  warped_img: ndarray
      Rectified image.
  &#34;&#34;&#34;
  if image.ndim != 2:
    raise ValueError

  # Compute all edgelets.
  edgelets1 = compute_edgelets(image)

  if algorithm == &#39;independent&#39;:
    # Find first vanishing point
    vp1 = ransac_vanishing_point(edgelets1, 2000, threshold_inlier=5)
    if reestimate:
      vp1 = reestimate_model(vp1, edgelets1, 5)

    # Remove inlier to remove dominating direction.
    edgelets2 = remove_inliers(vp1, edgelets1, 10)

    # Find second vanishing point
    vp2 = ransac_vanishing_point(edgelets2, 2000, threshold_inlier=5)
    if reestimate:
      vp2 = reestimate_model(vp2, edgelets2, 5)

  elif algorithm == &#39;3-line&#39;:
    focal_length = None
    vp1, vp2 = ransac_3_line(edgelets1,
                             focal_length,
                             num_ransac_iter=3000,
                             threshold_inlier=5)
  else:
    raise KeyError(
        &#34;Parameter &#39;algorithm&#39; has to be one of {&#39;3-line&#39;, &#39;independent&#39;}&#34;)

  # Compute the homography and warp
  warped_img = compute_homography_and_warp(image,
                                           vp1,
                                           vp2,
                                           clip_factor=clip_factor)

  return warped_img</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pano.distortion.rectification.compute_edgelets"><code class="name flex">
<span>def <span class="ident">compute_edgelets</span></span>(<span>image, sigma=3)</span>
</code></dt>
<dd>
<div class="desc"><p>Create edgelets as in the paper.</p>
<p>Uses canny edge detection and then finds (small) lines using probabilistic
hough transform as edgelets.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>image</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Image for which edgelets are to be computed.</dd>
<dt><strong><code>sigma</code></strong> :&ensp;<code>float</code></dt>
<dd>Smoothing to be used for canny edge detection.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>locations</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_edgelets, 2)</code></dt>
<dd>Locations of each of the edgelets.</dd>
<dt><strong><code>directions</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_edgelets, 2)</code></dt>
<dd>Direction of the edge (tangent) at each of the edgelet.</dd>
<dt><strong><code>strengths</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_edgelets,)</code></dt>
<dd>Length of the line segments detected for the edgelet.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_edgelets(image, sigma=3):
  &#34;&#34;&#34;Create edgelets as in the paper.

  Uses canny edge detection and then finds (small) lines using probabilistic
  hough transform as edgelets.

  Parameters
  ----------
  image: ndarray
      Image for which edgelets are to be computed.
  sigma: float
      Smoothing to be used for canny edge detection.

  Returns
  -------
  locations: ndarray of shape (n_edgelets, 2)
      Locations of each of the edgelets.
  directions: ndarray of shape (n_edgelets, 2)
      Direction of the edge (tangent) at each of the edgelet.
  strengths: ndarray of shape (n_edgelets,)
      Length of the line segments detected for the edgelet.
  &#34;&#34;&#34;
  edges = canny(image, sigma)
  lines = probabilistic_hough_line(edges, line_length=3, line_gap=2)
  lines = np.array(lines)

  locations = np.average(lines, axis=1)
  directions = lines[:, 1, :] - lines[:, 0, :]
  strengths = np.linalg.norm(directions, ord=2, axis=1)
  directions = np.divide(directions, strengths.reshape([-1, 1]))

  return (locations, directions, strengths)</code></pre>
</details>
</dd>
<dt id="pano.distortion.rectification.compute_homography_and_warp"><code class="name flex">
<span>def <span class="ident">compute_homography_and_warp</span></span>(<span>image, vp1, vp2, clip=True, clip_factor=3)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute homography from vanishing points and warp the image.</p>
<p>It is assumed that vp1 and vp2 correspond to horizontal and vertical
directions, although the order is not assumed.
Firstly, projective transform is computed to make the vanishing points go
to infinity so that we have a fronto parallel view. Then, Computes affine
transform
to make axes corresponding to vanishing points orthogonal.
Finally, Image is translated so that the image is not missed. Note that
this image can be very large. <code>clip</code> is provided to deal with this.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>image</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Image which has to be wrapped.</dd>
<dt><strong><code>vp1</code></strong> :&ensp;<code>ndarray</code> of <code>shape (3, )</code></dt>
<dd>First vanishing point in homogenous coordinate system.</dd>
<dt><strong><code>vp2</code></strong> :&ensp;<code>ndarray</code> of <code>shape (3, )</code></dt>
<dd>Second vanishing point in homogenous coordinate system.</dd>
<dt><strong><code>clip</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If True, image is clipped to clip_factor.</dd>
<dt><strong><code>clip_factor</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Proportion of image in multiples of image size to be retained if gone
out of bounds after homography.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>warped_img</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Image warped using homography as described above.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_homography_and_warp(image, vp1, vp2, clip=True, clip_factor=3):
  &#34;&#34;&#34;Compute homography from vanishing points and warp the image.

  It is assumed that vp1 and vp2 correspond to horizontal and vertical
  directions, although the order is not assumed.
  Firstly, projective transform is computed to make the vanishing points go
  to infinity so that we have a fronto parallel view. Then, Computes affine
  transform  to make axes corresponding to vanishing points orthogonal.
  Finally, Image is translated so that the image is not missed. Note that
  this image can be very large. `clip` is provided to deal with this.

  Parameters
  ----------
  image: ndarray
      Image which has to be wrapped.
  vp1: ndarray of shape (3, )
      First vanishing point in homogenous coordinate system.
  vp2: ndarray of shape (3, )
      Second vanishing point in homogenous coordinate system.
  clip: bool, optional
      If True, image is clipped to clip_factor.
  clip_factor: float, optional
      Proportion of image in multiples of image size to be retained if gone
      out of bounds after homography.

  Returns
  -------
  warped_img: ndarray
      Image warped using homography as described above.
  &#34;&#34;&#34;
  # Find Projective Transform
  vanishing_line = np.cross(vp1, vp2)
  H = np.eye(3)
  H[2] = vanishing_line / vanishing_line[2]
  H = H / H[2, 2]

  # Find directions corresponding to vanishing points
  v_post1 = np.dot(H, vp1)
  v_post2 = np.dot(H, vp2)
  v_post1 = v_post1 / np.sqrt(v_post1[0]**2 + v_post1[1]**2)
  v_post2 = v_post2 / np.sqrt(v_post2[0]**2 + v_post2[1]**2)

  directions = np.array([[v_post1[0], -v_post1[0], v_post2[0], -v_post2[0]],
                         [v_post1[1], -v_post1[1], v_post2[1], -v_post2[1]]])

  thetas = np.arctan2(directions[0], directions[1])

  # Find direction closest to horizontal axis
  h_ind = np.argmin(np.abs(thetas))

  # Find positive angle among the rest for the vertical axis
  if h_ind // 2 == 0:
    v_ind = 2 + np.argmax([thetas[2], thetas[3]])
  else:
    v_ind = np.argmax([thetas[2], thetas[3]])

  A1 = np.array([
      [directions[0, v_ind], directions[0, h_ind], 0],
      [directions[1, v_ind], directions[1, h_ind], 0],
      [0, 0, 1],
  ])
  # Might be a reflection. If so, remove reflection.
  if np.linalg.det(A1) &lt; 0:
    A1[:, 0] = -A1[:, 0]

  A = np.linalg.inv(A1)

  # Translate so that whole of the image is covered
  inter_matrix = np.dot(A, H)

  cords = np.dot(inter_matrix,
                 [[0, 0, image.shape[1], image.shape[1]],
                  [0, image.shape[0], 0, image.shape[0]], [1, 1, 1, 1]])
  cords = cords[:2] / cords[2]

  tx = min(0, cords[0].min())
  ty = min(0, cords[1].min())

  max_x = cords[0].max() - tx
  max_y = cords[1].max() - ty

  if clip:
    # These might be too large. Clip them.
    max_offset = max(image.shape) * clip_factor / 2
    tx = max(tx, -max_offset)
    ty = max(ty, -max_offset)

    max_x = min(max_x, -tx + max_offset)
    max_y = min(max_y, -ty + max_offset)

  max_x = int(max_x)
  max_y = int(max_y)

  T = np.array([[1, 0, -tx], [0, 1, -ty], [0, 0, 1]])

  final_homography = np.dot(T, inter_matrix)

  warped_img = warp(image,
                    np.linalg.inv(final_homography),
                    output_shape=(max_y, max_x))

  return warped_img</code></pre>
</details>
</dd>
<dt id="pano.distortion.rectification.compute_votes"><code class="name flex">
<span>def <span class="ident">compute_votes</span></span>(<span>edgelets, model, threshold_inlier=5)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute votes for each of the edgelet against a given vanishing point.</p>
<p>Votes for edgelets which lie inside threshold are same as their strengths,
otherwise zero.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>edgelets</code></strong> :&ensp;<code>tuple</code> of <code>ndarrays</code></dt>
<dd>(locations, directions, strengths) as computed by <code><a title="pano.distortion.rectification.compute_edgelets" href="#pano.distortion.rectification.compute_edgelets">compute_edgelets()</a></code>.</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>ndarray</code> of <code>shape (3,)</code></dt>
<dd>Vanishing point model in homogenous cordinate system.</dd>
<dt><strong><code>threshold_inlier</code></strong> :&ensp;<code>float</code></dt>
<dd>Threshold to be used for computing inliers in degrees. Angle between
edgelet direction and line connecting the
Vanishing point model and
edgelet location is used to threshold.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>votes</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_edgelets,)</code></dt>
<dd>Votes towards vanishing point model for each of the edgelet.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_votes(edgelets, model, threshold_inlier=5):
  &#34;&#34;&#34;Compute votes for each of the edgelet against a given vanishing point.

  Votes for edgelets which lie inside threshold are same as their strengths,
  otherwise zero.

  Parameters
  ----------
  edgelets: tuple of ndarrays
      (locations, directions, strengths) as computed by `compute_edgelets`.
  model: ndarray of shape (3,)
      Vanishing point model in homogenous cordinate system.
  threshold_inlier: float
      Threshold to be used for computing inliers in degrees. Angle between
      edgelet direction and line connecting the  Vanishing point model and
      edgelet location is used to threshold.

  Returns
  -------
  votes: ndarray of shape (n_edgelets,)
      Votes towards vanishing point model for each of the edgelet.

  &#34;&#34;&#34;
  vp = model[:2] / model[2]

  locations, directions, strengths = edgelets

  est_directions = locations - vp
  dot_prod = np.sum(est_directions * directions, axis=1)
  abs_prod = (np.linalg.norm(directions, axis=1) *
              np.linalg.norm(est_directions, axis=1))
  abs_prod[abs_prod == 0] = 1e-5

  cosine_theta = np.clip(dot_prod / abs_prod, a_min=-1.0, a_max=1.0)
  theta = np.arccos(np.abs(cosine_theta))

  theta_thresh = threshold_inlier * np.pi / 180

  return (theta &lt; theta_thresh) * strengths</code></pre>
</details>
</dd>
<dt id="pano.distortion.rectification.edgelet_lines"><code class="name flex">
<span>def <span class="ident">edgelet_lines</span></span>(<span>edgelets)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute lines in homogenous system for edgelets.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>edgelets</code></strong> :&ensp;<code>tuple</code> of <code>ndarrays</code></dt>
<dd>(locations, directions, strengths) as computed by <code><a title="pano.distortion.rectification.compute_edgelets" href="#pano.distortion.rectification.compute_edgelets">compute_edgelets()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>lines</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_edgelets, 3)</code></dt>
<dd>Lines at each of edgelet locations in homogenous system.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def edgelet_lines(edgelets):
  &#34;&#34;&#34;Compute lines in homogenous system for edgelets.

  Parameters
  ----------
  edgelets: tuple of ndarrays
      (locations, directions, strengths) as computed by `compute_edgelets`.

  Returns
  -------
  lines: ndarray of shape (n_edgelets, 3)
      Lines at each of edgelet locations in homogenous system.
  &#34;&#34;&#34;
  locations, directions, _ = edgelets
  normals = np.zeros_like(directions)
  normals[:, 0] = directions[:, 1]
  normals[:, 1] = -directions[:, 0]
  p = -np.sum(locations * normals, axis=1)
  lines = np.concatenate((normals, p[:, np.newaxis]), axis=1)

  return lines</code></pre>
</details>
</dd>
<dt id="pano.distortion.rectification.ransac_3_line"><code class="name flex">
<span>def <span class="ident">ransac_3_line</span></span>(<span>edgelets, focal_length, num_ransac_iter=2000, threshold_inlier=5) ‑> Tuple[Optional[numpy.ndarray], Optional[numpy.ndarray]]</span>
</code></dt>
<dd>
<div class="desc"><p>Estimate orthogonal vanishing points using 3 line Ransac algorithm.</p>
<p>Assumes camera has been calibrated and its focal length is known.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>edgelets</code></strong> :&ensp;<code>tuple</code> of <code>ndarrays</code></dt>
<dd>(locations, directions, strengths) as computed by <code><a title="pano.distortion.rectification.compute_edgelets" href="#pano.distortion.rectification.compute_edgelets">compute_edgelets()</a></code>.</dd>
<dt><strong><code>focal_length</code></strong> :&ensp;<code>float</code></dt>
<dd>Focal length of the camera used.</dd>
<dt><strong><code>num_ransac_iter</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of iterations to run ransac.</dd>
<dt><strong><code>threshold_inlier</code></strong> :&ensp;<code>float</code></dt>
<dd>threshold to be used for computing inliers in degrees.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>vp1</code></strong> :&ensp;<code>ndarray</code> of <code>shape (3,)</code></dt>
<dd>Estimated model for first vanishing point.</dd>
<dt><strong><code>vp2</code></strong> :&ensp;<code>ndarray</code> of <code>shape (3,)</code></dt>
<dd>Estimated model for second vanishing point, which is orthogonal to
first vanishing point.</dd>
</dl>
<h2 id="reference">Reference</h2>
<p>Bazin, Jean-Charles, and Marc Pollefeys. "3-line RANSAC for orthogonal
vanishing point detection." 2012 IEEE/RSJ International Conference on
Intelligent Robots and Systems. IEEE, 2012.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ransac_3_line(
    edgelets,
    focal_length,
    num_ransac_iter=2000,
    threshold_inlier=5) -&gt; Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
  &#34;&#34;&#34;Estimate orthogonal vanishing points using 3 line Ransac algorithm.

  Assumes camera has been calibrated and its focal length is known.

  Parameters
  ----------
  edgelets: tuple of ndarrays
      (locations, directions, strengths) as computed by `compute_edgelets`.
  focal_length: float
      Focal length of the camera used.
  num_ransac_iter: int
      Number of iterations to run ransac.
  threshold_inlier: float
      threshold to be used for computing inliers in degrees.

  Returns
  -------
  vp1: ndarray of shape (3,)
      Estimated model for first vanishing point.
  vp2: ndarray of shape (3,)
      Estimated model for second vanishing point, which is orthogonal to
      first vanishing point.

  Reference
  ---------
  Bazin, Jean-Charles, and Marc Pollefeys. &#34;3-line RANSAC for orthogonal
  vanishing point detection.&#34; 2012 IEEE/RSJ International Conference on
  Intelligent Robots and Systems. IEEE, 2012.
  &#34;&#34;&#34;
  locations, directions, strengths = edgelets
  lines = edgelet_lines(edgelets)

  num_pts = strengths.size

  arg_sort = np.argsort(-strengths)
  first_index_space = arg_sort[:num_pts // 5]
  second_index_space = arg_sort[:num_pts // 5]
  third_index_space = arg_sort[:num_pts // 2]

  invfsq = 1 / (focal_length**2)
  farr = np.array([invfsq, invfsq, 1.0])

  best_model = (None, None)
  best_votes = 0

  for ransac_iter in range(num_ransac_iter):
    ind1 = np.random.choice(first_index_space)
    ind2 = np.random.choice(second_index_space)
    ind3 = np.random.choice(third_index_space)

    l1 = lines[ind1]
    l2 = lines[ind2]
    l3 = lines[ind3]

    vp1 = np.cross(l1, l2)

    # The vanishing line polar to v1
    # h = np.dot(vp1, [1 / focal_length**2, 1 / focal_length**2, 1])
    # h = [vp1[0] * invfsq, vp1[1] * invfsq, vp1[2]]
    h = vp1 * farr
    vp2 = np.cross(h, l3)

    if np.sum(vp1**2) &lt; 1 or vp1[2] == 0:
      # reject degenerate candidates
      continue

    if np.sum(vp2**2) &lt; 1 or vp2[2] == 0:
      # reject degenerate candidates
      continue

    vp1_votes = compute_votes(edgelets, vp1, threshold_inlier)
    vp2_votes = compute_votes(edgelets, vp2, threshold_inlier)
    current_votes = (vp1_votes &gt; 0).sum() + (vp2_votes &gt; 0).sum()

    if current_votes &gt; best_votes:
      best_model = (vp1, vp2)
      best_votes = current_votes
      logger.debug(&#34;Current best model has {:.2f} votes at iteration {}&#34;,
                   current_votes, ransac_iter)

  return best_model</code></pre>
</details>
</dd>
<dt id="pano.distortion.rectification.ransac_vanishing_point"><code class="name flex">
<span>def <span class="ident">ransac_vanishing_point</span></span>(<span>edgelets, num_ransac_iter=2000, threshold_inlier=5)</span>
</code></dt>
<dd>
<div class="desc"><p>Estimate vanishing point using Ransac.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>edgelets</code></strong> :&ensp;<code>tuple</code> of <code>ndarrays</code></dt>
<dd>(locations, directions, strengths) as computed by <code><a title="pano.distortion.rectification.compute_edgelets" href="#pano.distortion.rectification.compute_edgelets">compute_edgelets()</a></code>.</dd>
<dt><strong><code>num_ransac_iter</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of iterations to run ransac.</dd>
<dt><strong><code>threshold_inlier</code></strong> :&ensp;<code>float</code></dt>
<dd>threshold to be used for computing inliers in degrees.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>best_model</code></strong> :&ensp;<code>ndarray</code> of <code>shape (3,)</code></dt>
<dd>Best model for vanishing point estimated.</dd>
</dl>
<h2 id="reference">Reference</h2>
<p>Chaudhury, Krishnendu, Stephen DiVerdi, and Sergey Ioffe.
"Auto-rectification of user photos." 2014 IEEE International Conference on
Image Processing (ICIP). IEEE, 2014.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ransac_vanishing_point(edgelets, num_ransac_iter=2000, threshold_inlier=5):
  &#34;&#34;&#34;Estimate vanishing point using Ransac.

  Parameters
  ----------
  edgelets: tuple of ndarrays
      (locations, directions, strengths) as computed by `compute_edgelets`.
  num_ransac_iter: int
      Number of iterations to run ransac.
  threshold_inlier: float
      threshold to be used for computing inliers in degrees.

  Returns
  -------
  best_model: ndarray of shape (3,)
      Best model for vanishing point estimated.

  Reference
  ---------
  Chaudhury, Krishnendu, Stephen DiVerdi, and Sergey Ioffe.
  &#34;Auto-rectification of user photos.&#34; 2014 IEEE International Conference on
  Image Processing (ICIP). IEEE, 2014.
  &#34;&#34;&#34;
  locations, directions, strengths = edgelets
  lines = edgelet_lines(edgelets)

  num_pts = strengths.size

  arg_sort = np.argsort(-strengths)
  first_index_space = arg_sort[:num_pts // 5]
  second_index_space = arg_sort[:num_pts // 2]

  best_model = None
  best_votes = np.zeros(num_pts)

  for ransac_iter in range(num_ransac_iter):
    ind1 = np.random.choice(first_index_space)
    ind2 = np.random.choice(second_index_space)

    l1 = lines[ind1]
    l2 = lines[ind2]

    current_model = np.cross(l1, l2)

    if np.sum(current_model**2) &lt; 1 or current_model[2] == 0:
      # reject degenerate candidates
      continue

    current_votes = compute_votes(edgelets, current_model, threshold_inlier)

    if current_votes.sum() &gt; best_votes.sum():
      best_model = current_model
      best_votes = current_votes
      logger.trace(&#34;Current best model has {:.2f} votes at iteration {}&#34;,
                   current_votes.sum(), ransac_iter)

  logger.debug(&#39;Best model has {:.2f} votes&#39;, best_votes.sum())

  return best_model</code></pre>
</details>
</dd>
<dt id="pano.distortion.rectification.rectify_image"><code class="name flex">
<span>def <span class="ident">rectify_image</span></span>(<span>image: numpy.ndarray, clip_factor=6, algorithm='independent', reestimate=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Rectified image with vanishing point computed using ransac.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>image</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Image which has to be rectified. Must be gray.</dd>
<dt><strong><code>clip_factor</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Proportion of image in multiples of image size to be retained if gone
out of bounds after homography.</dd>
<dt><strong><code>algorithm</code></strong> :&ensp;<code>one</code> of <code>{'3-line', 'independent'}</code></dt>
<dd>independent ransac algorithm finds the orthogonal vanishing points by
applying ransac twice.
3-line algorithm finds the orthogonal vanishing points together, but
assumes knowledge of focal length.</dd>
<dt><strong><code>reestimate</code></strong> :&ensp;<code>bool</code></dt>
<dd>If ransac results are to be reestimated using least squares with
inliers. Turn this off if getting bad results.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>warped_img</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Rectified image.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rectify_image(image: np.ndarray,
                  clip_factor=6,
                  algorithm=&#39;independent&#39;,
                  reestimate=False):
  &#34;&#34;&#34;Rectified image with vanishing point computed using ransac.

  Parameters
  ----------
  image: ndarray
      Image which has to be rectified. Must be gray.
  clip_factor: float, optional
      Proportion of image in multiples of image size to be retained if gone
      out of bounds after homography.
  algorithm: one of {&#39;3-line&#39;, &#39;independent&#39;}
      independent ransac algorithm finds the orthogonal vanishing points by
      applying ransac twice.
      3-line algorithm finds the orthogonal vanishing points together, but
      assumes knowledge of focal length.
  reestimate: bool
      If ransac results are to be reestimated using least squares with
      inliers. Turn this off if getting bad results.

  Returns
  -------
  warped_img: ndarray
      Rectified image.
  &#34;&#34;&#34;
  if image.ndim != 2:
    raise ValueError

  # Compute all edgelets.
  edgelets1 = compute_edgelets(image)

  if algorithm == &#39;independent&#39;:
    # Find first vanishing point
    vp1 = ransac_vanishing_point(edgelets1, 2000, threshold_inlier=5)
    if reestimate:
      vp1 = reestimate_model(vp1, edgelets1, 5)

    # Remove inlier to remove dominating direction.
    edgelets2 = remove_inliers(vp1, edgelets1, 10)

    # Find second vanishing point
    vp2 = ransac_vanishing_point(edgelets2, 2000, threshold_inlier=5)
    if reestimate:
      vp2 = reestimate_model(vp2, edgelets2, 5)

  elif algorithm == &#39;3-line&#39;:
    focal_length = None
    vp1, vp2 = ransac_3_line(edgelets1,
                             focal_length,
                             num_ransac_iter=3000,
                             threshold_inlier=5)
  else:
    raise KeyError(
        &#34;Parameter &#39;algorithm&#39; has to be one of {&#39;3-line&#39;, &#39;independent&#39;}&#34;)

  # Compute the homography and warp
  warped_img = compute_homography_and_warp(image,
                                           vp1,
                                           vp2,
                                           clip_factor=clip_factor)

  return warped_img</code></pre>
</details>
</dd>
<dt id="pano.distortion.rectification.reestimate_model"><code class="name flex">
<span>def <span class="ident">reestimate_model</span></span>(<span>model, edgelets, threshold_reestimate=5)</span>
</code></dt>
<dd>
<div class="desc"><p>Reestimate vanishing point using inliers and least squares.</p>
<p>All the edgelets which are within a threshold are used to reestimate model</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>ndarray</code> of <code>shape (3,)</code></dt>
<dd>Vanishing point model in homogenous coordinates which is to be
reestimated.</dd>
<dt><strong><code>edgelets</code></strong> :&ensp;<code>tuple</code> of <code>ndarrays</code></dt>
<dd>(locations, directions, strengths) as computed by <code><a title="pano.distortion.rectification.compute_edgelets" href="#pano.distortion.rectification.compute_edgelets">compute_edgelets()</a></code>.
All edgelets from which inliers will be computed.</dd>
<dt><strong><code>threshold_inlier</code></strong> :&ensp;<code>float</code></dt>
<dd>threshold to be used for finding inlier edgelets.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>reestimated_model</code></strong> :&ensp;<code>ndarray</code> of <code>shape (3,)</code></dt>
<dd>Reestimated model for vanishing point in homogenous coordinates.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reestimate_model(model, edgelets, threshold_reestimate=5):
  &#34;&#34;&#34;Reestimate vanishing point using inliers and least squares.

  All the edgelets which are within a threshold are used to reestimate model

  Parameters
  ----------
  model: ndarray of shape (3,)
      Vanishing point model in homogenous coordinates which is to be
      reestimated.
  edgelets: tuple of ndarrays
      (locations, directions, strengths) as computed by `compute_edgelets`.
      All edgelets from which inliers will be computed.
  threshold_inlier: float
      threshold to be used for finding inlier edgelets.

  Returns
  -------
  reestimated_model: ndarray of shape (3,)
      Reestimated model for vanishing point in homogenous coordinates.
  &#34;&#34;&#34;
  locations, directions, strengths = edgelets

  inliers = compute_votes(edgelets, model, threshold_reestimate) &gt; 0
  locations = locations[inliers]
  directions = directions[inliers]
  strengths = strengths[inliers]

  lines = edgelet_lines((locations, directions, strengths))

  a = lines[:, :2]
  b = -lines[:, 2]
  est_model = np.linalg.lstsq(a, b)[0]

  return np.concatenate((est_model, [1.]))</code></pre>
</details>
</dd>
<dt id="pano.distortion.rectification.remove_inliers"><code class="name flex">
<span>def <span class="ident">remove_inliers</span></span>(<span>model, edgelets, threshold_inlier=10)</span>
</code></dt>
<dd>
<div class="desc"><p>Remove all inlier edgelets of a given model.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>ndarray</code> of <code>shape (3,)</code></dt>
<dd>Vanishing point model in homogenous coordinates which is to be
reestimated.</dd>
<dt><strong><code>edgelets</code></strong> :&ensp;<code>tuple</code> of <code>ndarrays</code></dt>
<dd>(locations, directions, strengths) as computed by <code><a title="pano.distortion.rectification.compute_edgelets" href="#pano.distortion.rectification.compute_edgelets">compute_edgelets()</a></code>.</dd>
<dt><strong><code>threshold_inlier</code></strong> :&ensp;<code>float</code></dt>
<dd>threshold to be used for finding inlier edgelets.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>edgelets_new</code></strong> :&ensp;<code>tuple</code> of <code>ndarrays</code></dt>
<dd>All Edgelets except those which are inliers to model.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_inliers(model, edgelets, threshold_inlier=10):
  &#34;&#34;&#34;Remove all inlier edgelets of a given model.

  Parameters
  ----------
  model: ndarray of shape (3,)
      Vanishing point model in homogenous coordinates which is to be
      reestimated.
  edgelets: tuple of ndarrays
      (locations, directions, strengths) as computed by `compute_edgelets`.
  threshold_inlier: float
      threshold to be used for finding inlier edgelets.

  Returns
  -------
  edgelets_new: tuple of ndarrays
      All Edgelets except those which are inliers to model.
  &#34;&#34;&#34;
  inliers = compute_votes(edgelets, model, threshold_inlier) &gt; 0
  locations, directions, strengths = edgelets
  locations = locations[~inliers]
  directions = directions[~inliers]
  strengths = strengths[~inliers]
  edgelets = (locations, directions, strengths)

  return edgelets</code></pre>
</details>
</dd>
<dt id="pano.distortion.rectification.vis_edgelets"><code class="name flex">
<span>def <span class="ident">vis_edgelets</span></span>(<span>image, edgelets, show=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper function to visualize edgelets.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vis_edgelets(image, edgelets, show=True):
  &#34;&#34;&#34;Helper function to visualize edgelets.&#34;&#34;&#34;
  import matplotlib.pyplot as plt

  plt.figure(figsize=(10, 10))
  plt.imshow(image)
  locations, directions, strengths = edgelets

  for i in range(locations.shape[0]):
    xax = [
        locations[i, 0] - directions[i, 0] * strengths[i] / 2,
        locations[i, 0] + directions[i, 0] * strengths[i] / 2
    ]
    yax = [
        locations[i, 1] - directions[i, 1] * strengths[i] / 2,
        locations[i, 1] + directions[i, 1] * strengths[i] / 2
    ]

    plt.plot(xax, yax, &#39;r-&#39;)

  if show:
    plt.show()</code></pre>
</details>
</dd>
<dt id="pano.distortion.rectification.vis_model"><code class="name flex">
<span>def <span class="ident">vis_model</span></span>(<span>image, model, show=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper function to visualize computed model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vis_model(image, model, show=True):
  &#34;&#34;&#34;Helper function to visualize computed model.&#34;&#34;&#34;
  import matplotlib.pyplot as plt

  edgelets = compute_edgelets(image)
  locations, directions, strengths = edgelets
  inliers = compute_votes(edgelets, model, 10) &gt; 0

  edgelets = (locations[inliers], directions[inliers], strengths[inliers])
  locations, directions, strengths = edgelets
  vis_edgelets(image, edgelets, False)
  vp = model / model[2]
  plt.plot(vp[0], vp[1], &#39;bo&#39;)

  for i in range(locations.shape[0]):
    xax = [locations[i, 0], vp[0]]
    yax = [locations[i, 1], vp[1]]
    plt.plot(xax, yax, &#39;b-.&#39;)

  if show:
    plt.show()</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#references">References</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pano.distortion" href="index.html">pano.distortion</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pano.distortion.rectification.compute_edgelets" href="#pano.distortion.rectification.compute_edgelets">compute_edgelets</a></code></li>
<li><code><a title="pano.distortion.rectification.compute_homography_and_warp" href="#pano.distortion.rectification.compute_homography_and_warp">compute_homography_and_warp</a></code></li>
<li><code><a title="pano.distortion.rectification.compute_votes" href="#pano.distortion.rectification.compute_votes">compute_votes</a></code></li>
<li><code><a title="pano.distortion.rectification.edgelet_lines" href="#pano.distortion.rectification.edgelet_lines">edgelet_lines</a></code></li>
<li><code><a title="pano.distortion.rectification.ransac_3_line" href="#pano.distortion.rectification.ransac_3_line">ransac_3_line</a></code></li>
<li><code><a title="pano.distortion.rectification.ransac_vanishing_point" href="#pano.distortion.rectification.ransac_vanishing_point">ransac_vanishing_point</a></code></li>
<li><code><a title="pano.distortion.rectification.rectify_image" href="#pano.distortion.rectification.rectify_image">rectify_image</a></code></li>
<li><code><a title="pano.distortion.rectification.reestimate_model" href="#pano.distortion.rectification.reestimate_model">reestimate_model</a></code></li>
<li><code><a title="pano.distortion.rectification.remove_inliers" href="#pano.distortion.rectification.remove_inliers">remove_inliers</a></code></li>
<li><code><a title="pano.distortion.rectification.vis_edgelets" href="#pano.distortion.rectification.vis_edgelets">vis_edgelets</a></code></li>
<li><code><a title="pano.distortion.rectification.vis_model" href="#pano.distortion.rectification.vis_model">vis_model</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>